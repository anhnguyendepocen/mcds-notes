\chapter{Gradient Boosting \label{chapter:gradientboosting}}

For several years, no one knew why AdaBoost (Chapter~\ref{chapter:boosting}) worked. Then, in 2001, Jerome Friedman published a paper on \textbf{gradient boosting}, which generalized the boosting idea to many problem classes. In the gradient boosting framework, of which AdaBoost is a subclass, there are three components:

\begin{enumerate}
\item A loss function to be optimized.
\item Weak learners to make predictions.
\item An additive model that adds the contributions of different weak learners to minimize the loss function.
\end{enumerate}

Given a predefined loss function and a method for building new weak learners, gradient boosting provides a principled way to create new learners and add them to the combined model. It can be used to boost learners for a wide array of different objective functions. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gradient Boosting for Classification}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gradient Boosting for Regression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Gradient Boosting for Survival Analysis}